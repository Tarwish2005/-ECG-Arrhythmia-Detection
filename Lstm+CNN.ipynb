{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tarwish2005/-ECG-Arrhythmia-Detection/blob/main/Lstm%2BCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install wfdb numpy scipy scikit-learn torch torchaudio torchvision matplotlib\n"
      ],
      "metadata": {
        "id": "mQ5ofpTaiJxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a62c4b9c-6bae-47df-88de-c5be21eb4cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed (e.g., in Colab):\n",
        "# !pip install wfdb numpy scipy scikit-learn torch torchvision matplotlib\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.signal import butter, filtfilt, resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n"
      ],
      "metadata": {
        "id": "QtLzkzWuqGnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameter"
      ],
      "metadata": {
        "id": "dMorYJMglExF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VFDB_DIR   = \"/content/drive/MyDrive/ECG/VFDB\"\n",
        "CUVTDB_DIR = \"/content/drive/MyDrive/ECG/CUVTDB\"\n",
        "ADB_DIR    = \"/content/drive/MyDrive/ECG/MITDB\"   # (use Arrhythmia DB path if different)\n",
        "NSTDB_DIR  = \"/content/drive/MyDrive/ECG/NSTDB\"       # contains 'ma' noise\n",
        "TARGET_FS  = 250\n",
        "WIN_SEC    = 5\n",
        "WIN_SAMPLES = TARGET_FS * WIN_SEC\n"
      ],
      "metadata": {
        "id": "7uFlFfi6iNpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing\n"
      ],
      "metadata": {
        "id": "6UxGUEwslJLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def highpass_filter(signal, fs=250, cutoff=1.0, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype=\"high\", analog=False)\n",
        "    return filtfilt(b, a, signal)\n",
        "\n",
        "def preprocess_ecg(signal, fs=250, target_fs=250, window_size=5):\n",
        "    # 1) resample\n",
        "    if fs != target_fs:\n",
        "        n_samples = int(len(signal) * target_fs / fs)\n",
        "        signal = resample(signal, n_samples)\n",
        "\n",
        "    # 2) remove baseline wander\n",
        "    signal = highpass_filter(signal, fs=target_fs, cutoff=1.0, order=4)\n",
        "\n",
        "    # 3) z-score normalize\n",
        "    mu, sigma = np.mean(signal), np.std(signal) + 1e-8\n",
        "    signal = (signal - mu) / sigma\n",
        "\n",
        "    # 4) segment into non-overlapping windows\n",
        "    segment_length = window_size * target_fs\n",
        "    n_segments = len(signal) // segment_length\n",
        "    if n_segments == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    segments = []\n",
        "    for i in range(n_segments):\n",
        "        seg = signal[i*segment_length:(i+1)*segment_length]\n",
        "        if not (np.isnan(seg).any() or np.isinf(seg).any()):\n",
        "            segments.append(seg)\n",
        "\n",
        "    return np.asarray(segments)\n"
      ],
      "metadata": {
        "id": "M5Rryzjbim2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA"
      ],
      "metadata": {
        "id": "WOFHqozFyRMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _list_records(db_path):\n",
        "    return [f.split('.')[0] for f in os.listdir(db_path) if f.endswith('.dat')]\n",
        "\n",
        "def load_mitdb(db_path):\n",
        "    X, y = [], []\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"Warning: {db_path} does not exist\"); return X, y\n",
        "    records = _list_records(db_path)\n",
        "    print(f\"Found {len(records)} MITDB records\")\n",
        "\n",
        "    for rec_name in records:\n",
        "        try:\n",
        "            rec = wfdb.rdrecord(os.path.join(db_path, rec_name))\n",
        "            if rec.p_signal is not None and rec.p_signal.shape[1] > 0:\n",
        "                sig = rec.p_signal[:, 0]\n",
        "                segments = preprocess_ecg(sig, fs=rec.fs)\n",
        "                if len(segments) > 0:\n",
        "                    X.extend(segments)     # Non-VT/VF segments\n",
        "                    y.extend([0]*len(segments))\n",
        "        except Exception as e:\n",
        "            print(f\"MITDB error {rec_name}: {e}\")\n",
        "    print(f\"MITDB: {len(X)} segments\")\n",
        "    return X, y\n",
        "\n",
        "def load_vfdb(db_path):\n",
        "    X, y = [], []\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"Warning: {db_path} does not exist\"); return X, y\n",
        "    records = _list_records(db_path)\n",
        "    print(f\"Found {len(records)} VFDB records\")\n",
        "\n",
        "    for rec_name in records:\n",
        "        try:\n",
        "            rec = wfdb.rdrecord(os.path.join(db_path, rec_name))\n",
        "            if rec.p_signal is not None and rec.p_signal.shape[1] > 0:\n",
        "                sig = rec.p_signal[:, 0]\n",
        "                segments = preprocess_ecg(sig, fs=rec.fs)\n",
        "                if len(segments) > 0:\n",
        "                    X.extend(segments)     # VT/VF segments\n",
        "                    y.extend([1]*len(segments))\n",
        "        except Exception as e:\n",
        "            print(f\"VFDB error {rec_name}: {e}\")\n",
        "    print(f\"VFDB: {len(X)} segments\")\n",
        "    return X, y\n",
        "\n",
        "def load_cuvtdb(db_path):\n",
        "    X, y = [], []\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"Warning: {db_path} does not exist\"); return X, y\n",
        "    records = _list_records(db_path)\n",
        "    print(f\"Found {len(records)} CUVTDB records\")\n",
        "\n",
        "    for rec_name in records:\n",
        "        try:\n",
        "            rec = wfdb.rdrecord(os.path.join(db_path, rec_name))\n",
        "            if rec.p_signal is not None and rec.p_signal.shape[1] > 0:\n",
        "                sig = rec.p_signal[:, 0]\n",
        "                segments = preprocess_ecg(sig, fs=rec.fs)\n",
        "                if len(segments) > 0:\n",
        "                    X.extend(segments)     # VT/VF segments\n",
        "                    y.extend([1]*len(segments))\n",
        "        except Exception as e:\n",
        "            print(f\"CUVTDB error {rec_name}: {e}\")\n",
        "    print(f\"CUVTDB: {len(X)} segments\")\n",
        "    return X, y\n",
        "\n",
        "def load_nstdb_artifacts(db_path, target_fs=250, window_size=5):\n",
        "    \"\"\"\n",
        "    Return artifact windows from NSTDB to use as 'ma' (muscle artifact) sources.\n",
        "    We do NOT label them here; they’re used to synthesize the Noisy class.\n",
        "    \"\"\"\n",
        "    MA = []\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"Warning: {db_path} does not exist\"); return MA\n",
        "\n",
        "    records = _list_records(db_path)\n",
        "    print(f\"Found {len(records)} NSTDB records (as artifact sources)\")\n",
        "\n",
        "    for rec_name in records:\n",
        "        try:\n",
        "            rec = wfdb.rdrecord(os.path.join(db_path, rec_name))\n",
        "            if rec.p_signal is not None and rec.p_signal.shape[1] > 0:\n",
        "                sig = rec.p_signal[:, 0]\n",
        "                # NOTE: We do minimal preprocessing (no high-pass), since it’s artifact\n",
        "                # but we standardize to target_fs and windowing to align lengths.\n",
        "                if rec.fs != target_fs:\n",
        "                    n_samples = int(len(sig) * target_fs / rec.fs)\n",
        "                    sig = resample(sig, n_samples)\n",
        "                # z-norm artifact\n",
        "                sig = (sig - np.mean(sig)) / (np.std(sig) + 1e-8)\n",
        "\n",
        "                seg_len = window_size * target_fs\n",
        "                n_segments = len(sig) // seg_len\n",
        "                for i in range(n_segments):\n",
        "                    MA.append(sig[i*seg_len:(i+1)*seg_len])\n",
        "        except Exception as e:\n",
        "            print(f\"NSTDB error {rec_name}: {e}\")\n",
        "    print(f\"NSTDB artifact windows: {len(MA)}\")\n",
        "    return np.asarray(MA)\n"
      ],
      "metadata": {
        "id": "VI0laye7yVhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "VFDB_DIR   = \"/content/drive/MyDrive/ECG/VFDB\"\n",
        "CUVTDB_DIR = \"/content/drive/MyDrive/ECG/CUVTDB\"\n",
        "ADB_DIR    = \"/content/drive/MyDrive/ECG/MITDB\"\n",
        "NSTDB_DIR  = \"/content/drive/MyDrive/ECG/NSTDB\"\n",
        "\n",
        "# Load base datasets\n",
        "non_vf_X, non_vf_y = load_mitdb(ADB_DIR)        # class 0\n",
        "vf1_X, vf1_y       = load_vfdb(VFDB_DIR)        # class 1\n",
        "vf2_X, vf2_y       = load_cuvtdb(CUVTDB_DIR)    # class 1\n",
        "ma_X               = load_nstdb_artifacts(NSTDB_DIR)  # unlabeled artifact windows\n",
        "\n",
        "# Combine VT/VF\n",
        "vf_X = vf1_X + vf2_X\n",
        "vf_y = vf1_y + vf2_y\n",
        "\n",
        "print(f\"Non-VF segments: {len(non_vf_X)} | VF segments: {len(vf_X)} | MA windows: {len(ma_X)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiFGjC8tWu9R",
        "outputId": "1ceb04d3-508a-4ecd-ce8a-cfe2eee38538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 48 MITDB records\n",
            "MITDB: 17328 segments\n",
            "Found 22 VFDB records\n",
            "VFDB: 9240 segments\n",
            "Found 39 CUVTDB records\n",
            "CUVTDB: 707 segments\n",
            "Found 30 NSTDB records (as artifact sources)\n",
            "NSTDB artifact windows: 10830\n",
            "Non-VF segments: 17328 | VF segments: 9947 | MA windows: 10830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(42)\n",
        "\n",
        "def add_gaussian_noise(seg, scale=0.05):\n",
        "    noise = rng.normal(0.0, 1.0, size=seg.shape)\n",
        "    return seg + scale * noise\n",
        "\n",
        "def synth_noisy_from_nonvf(seg, ma_bank, a=0.3, gauss_scale=0.08):\n",
        "    # pick a random MA window, same length\n",
        "    ma = ma_bank[rng.integers(0, len(ma_bank))]\n",
        "    ma = ma[:len(seg)]\n",
        "    # Non-VT/VF_noisy = Non-VT/VF_clean + a·w_n + ma\n",
        "    out = seg.copy()\n",
        "    out = out + a * rng.normal(0.0, 1.0, size=seg.shape)  # a·w_n\n",
        "    out = out + ma                                        # + ma\n",
        "    return add_gaussian_noise(out, scale=gauss_scale)     # extra small Gaussian\n",
        "\n",
        "def augment_vf(seg, b=0.02):\n",
        "    # VT/VF_aug = VT/VF_clean + b·w_n   (small)\n",
        "    return add_gaussian_noise(seg, scale=b)\n"
      ],
      "metadata": {
        "id": "NEgrH2-yWzkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to arrays\n",
        "non_vf_X = np.asarray(non_vf_X)\n",
        "vf_X     = np.asarray(vf_X)\n",
        "\n",
        "print(\"Shapes → Non-VF:\", non_vf_X.shape, \"| VF:\", vf_X.shape)\n",
        "\n",
        "# Create synthetic Noisy class from Non-VT/VF + Gaussian + MA\n",
        "noisy_syn = []\n",
        "for seg in non_vf_X:\n",
        "    noisy_syn.append(synth_noisy_from_nonvf(seg, ma_X, a=0.3, gauss_scale=0.08))\n",
        "noisy_syn = np.asarray(noisy_syn)\n",
        "\n",
        "# Augment VF with light Gaussian\n",
        "vf_aug = []\n",
        "for seg in vf_X:\n",
        "    vf_aug.append(augment_vf(seg, b=0.02))\n",
        "vf_aug = np.asarray(vf_aug)\n",
        "\n",
        "# Assemble X, y\n",
        "X = np.concatenate([non_vf_X, vf_X, noisy_syn, vf_aug], axis=0)\n",
        "y = np.concatenate([\n",
        "    np.zeros(len(non_vf_X), dtype=int),   # class 0: Non-VF\n",
        "    np.ones(len(vf_X), dtype=int),        # class 1: VF\n",
        "    2*np.ones(len(noisy_syn), dtype=int), # class 2: Noisy (synthetic)\n",
        "    np.ones(len(vf_aug), dtype=int),      # class 1: VF (augmented)\n",
        "])\n",
        "\n",
        "print(\"Final X:\", X.shape, \"| y counts:\", dict(zip(*np.unique(y, return_counts=True))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUYoRWRFW5WM",
        "outputId": "2fedacbd-0a3e-4e07-fba4-0f8615330073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes → Non-VF: (17328, 1250) | VF: (9947, 1250)\n",
            "Final X: (54550, 1250) | y counts: {np.int64(0): np.int64(17328), np.int64(1): np.int64(19894), np.int64(2): np.int64(17328)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
        "\n",
        "# To tensors (shape: [N, 1, 1250])\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_val_t   = torch.tensor(X_val,   dtype=torch.float32).unsqueeze(1)\n",
        "X_test_t  = torch.tensor(X_test,  dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
        "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
        "\n",
        "train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "val_ds   = TensorDataset(X_val_t,   y_val_t)\n",
        "test_ds  = TensorDataset(X_test_t,  y_test_t)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgtqgCFjXHLM",
        "outputId": "c0398395-d28d-45e7-cd7a-ffd6bed01610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 38185 | Val: 8182 | Test: 8183\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "aCyaaADIl9P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ECG_CNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_length=1250, num_classes=3, hidden_size=128, lstm_layers=2):\n",
        "        super().__init__()\n",
        "        # CNN branch\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32); self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64); self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128); self.pool3 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(256); self.pool4 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv5 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn5 = nn.BatchNorm1d(512); self.pool5 = nn.MaxPool1d(2)\n",
        "\n",
        "        # 1250 → 625 → 312 → 156 → 78 → 39\n",
        "        self.flattened_size = 512 * 39\n",
        "\n",
        "        # LSTM branch (bidirectional)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=1, hidden_size=hidden_size,\n",
        "            num_layers=lstm_layers, batch_first=True, bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Fusion + FC\n",
        "        self.fc1 = nn.Linear(self.flattened_size + 2*hidden_size, 256)\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.drop2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 1, L]\n",
        "        # CNN branch\n",
        "        c = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
        "        c = self.pool2(self.relu(self.bn2(self.conv2(c))))\n",
        "        c = self.pool3(self.relu(self.bn3(self.conv3(c))))\n",
        "        c = self.pool4(self.relu(self.bn4(self.conv4(c))))\n",
        "        c = self.pool5(self.relu(self.bn5(self.conv5(c))))\n",
        "        c = c.view(c.size(0), -1)  # [B, 512*39]\n",
        "\n",
        "        # LSTM branch (expects [B, L, 1])\n",
        "        l = x.permute(0, 2, 1)\n",
        "        l, _ = self.lstm(l)\n",
        "        l = l[:, -1, :]  # last time step, [B, 2*hidden]\n",
        "\n",
        "        # Fuse\n",
        "        f = torch.cat([c, l], dim=1)\n",
        "        features = self.relu(self.fc1(f))\n",
        "        features = self.drop1(features)\n",
        "        z = self.relu(self.fc2(features))\n",
        "        z = self.drop2(z)\n",
        "        logits = self.fc3(z)\n",
        "        return logits, features\n"
      ],
      "metadata": {
        "id": "jT0ndDb9l_c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ECG_CNN_LSTM(input_length=X.shape[1], num_classes=3, hidden_size=128, lstm_layers=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "epochs = 100\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "def accuracy_from_logits(logits, y_true):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == y_true).float().mean().item()\n",
        "\n",
        "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    model.train()\n",
        "    tloss, tcorrect, tcount = 0.0, 0, 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        tloss += loss.item()\n",
        "        tcorrect += (logits.argmax(1) == yb).sum().item()\n",
        "        tcount += yb.size(0)\n",
        "\n",
        "    # val\n",
        "    model.eval()\n",
        "    vloss, vcorrect, vcount = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits, _ = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            vloss += loss.item()\n",
        "            vcorrect += (logits.argmax(1) == yb).sum().item()\n",
        "            vcount += yb.size(0)\n",
        "\n",
        "    train_losses.append(tloss/len(train_loader))\n",
        "    val_losses.append(vloss/len(val_loader))\n",
        "    train_accs.append(tcorrect/tcount)\n",
        "    val_accs.append(vcorrect/vcount)\n",
        "\n",
        "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
        "              f\"Train Loss {train_losses[-1]:.4f} Acc {train_accs[-1]:.4f} | \"\n",
        "              f\"Val Loss {val_losses[-1]:.4f} Acc {val_accs[-1]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTLtzw4UNqtN",
        "outputId": "c7c5cadb-0322-4bfe-fb19-6b8bdcc40cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters: 6,265,603\n",
            "Epoch 001/100 | Train Loss 0.1799 Acc 0.9369 | Val Loss 0.0495 Acc 0.9853\n",
            "Epoch 010/100 | Train Loss 0.0267 Acc 0.9935 | Val Loss 0.0145 Acc 0.9958\n",
            "Epoch 020/100 | Train Loss 0.0134 Acc 0.9964 | Val Loss 0.0469 Acc 0.9918\n",
            "Epoch 030/100 | Train Loss 0.0127 Acc 0.9969 | Val Loss 0.0072 Acc 0.9979\n",
            "Epoch 040/100 | Train Loss 0.0108 Acc 0.9973 | Val Loss 0.0203 Acc 0.9938\n",
            "Epoch 050/100 | Train Loss 0.0119 Acc 0.9969 | Val Loss 0.0074 Acc 0.9974\n",
            "Epoch 060/100 | Train Loss 0.0114 Acc 0.9972 | Val Loss 0.0083 Acc 0.9983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptHX_vgJySRw",
        "outputId": "21617d88-2ea5-4ef9-cfe6-09fcd4bbe1cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Val Loss\")\n",
        "plt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_accs, label=\"Train Acc\")\n",
        "plt.plot(val_accs, label=\"Val Acc\")\n",
        "plt.title(\"Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KSXYdWsTs6ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "92exMAG6Y5uM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}