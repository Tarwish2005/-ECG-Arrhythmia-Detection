{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tarwish2005/-ECG-Arrhythmia-Detection/blob/main/The_Best_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install wfdb numpy scipy scikit-learn torch torchaudio torchvision matplotlib\n"
      ],
      "metadata": {
        "id": "mQ5ofpTaiJxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.signal import resample, butter, filtfilt\n",
        "import numpy as np\n",
        "import os\n",
        "import wfdb\n"
      ],
      "metadata": {
        "id": "QtLzkzWuqGnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameter"
      ],
      "metadata": {
        "id": "dMorYJMglExF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VFDB_DIR   = \"/content/drive/MyDrive/ECG/VFDB\"\n",
        "CUVTDB_DIR = \"/content/drive/MyDrive/ECG/CUVTDB\"\n",
        "ADB_DIR    = \"/content/drive/MyDrive/ECG/MITDB\"   # (use Arrhythmia DB path if different)\n",
        "NSTDB_DIR  = \"/content/drive/MyDrive/ECG/NSTDB\"       # contains 'ma' noise\n",
        "TARGET_FS  = 250\n",
        "WIN_SEC    = 5\n",
        "WIN_SAMPLES = TARGET_FS * WIN_SEC\n"
      ],
      "metadata": {
        "id": "7uFlFfi6iNpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing\n"
      ],
      "metadata": {
        "id": "6UxGUEwslJLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def highpass_filter(signal, fs=250, cutoff=1.0, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
        "    return filtfilt(b, a, signal)\n",
        "\n",
        "def preprocess_ecg(signal, fs=250, target_fs=250, window_size=5):\n",
        "    # Resample\n",
        "    if fs != target_fs:\n",
        "        signal = resample(signal, int(len(signal) * target_fs / fs))\n",
        "\n",
        "    # Baseline wander removal\n",
        "    signal = highpass_filter(signal, fs=target_fs)\n",
        "\n",
        "    # Mean subtraction + normalization\n",
        "    signal = signal - np.mean(signal)\n",
        "    std_val = np.std(signal)\n",
        "    if std_val > 1e-8:  # Avoid division by very small numbers\n",
        "        signal = signal / std_val\n",
        "\n",
        "    # Segment into windows\n",
        "    segment_length = window_size * target_fs\n",
        "    n_segments = len(signal) // segment_length\n",
        "\n",
        "    if n_segments == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    segments = []\n",
        "    for i in range(n_segments):\n",
        "        segment = signal[i*segment_length:(i+1)*segment_length]\n",
        "        # Additional check for NaN or inf values\n",
        "        if not (np.isnan(segment).any() or np.isinf(segment).any()):\n",
        "            segments.append(segment)\n",
        "\n",
        "    return np.array(segments)\n",
        "\n"
      ],
      "metadata": {
        "id": "M5Rryzjbim2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA"
      ],
      "metadata": {
        "id": "WOFHqozFyRMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mitdb(db_path):\n",
        "    X, y = [], []\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"Warning: {db_path} does not exist\")\n",
        "        return X, y\n",
        "\n",
        "    records = [f.split('.')[0] for f in os.listdir(db_path) if f.endswith('.dat')]\n",
        "    print(f\"Found {len(records)} MITDB records\")\n",
        "\n",
        "    for record in records:\n",
        "        try:\n",
        "            rec = wfdb.rdrecord(os.path.join(db_path, record))\n",
        "            if rec.p_signal is not None and rec.p_signal.shape[1] > 0:\n",
        "                sig = rec.p_signal[:, 0]  # single channel\n",
        "                segments = preprocess_ecg(sig, fs=rec.fs)\n",
        "                if len(segments) > 0:\n",
        "                    X.extend(segments)\n",
        "                    y.extend([0] * len(segments))  # Non-VT/VF\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {record}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"MITDB: Loaded {len(X)} segments\")\n",
        "    return X, y\n",
        "\n",
        "def load_vfdb(db_path):\n",
        "    X, y = [], []\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"Warning: {db_path} does not exist\")\n",
        "        return X, y\n",
        "\n",
        "    records = [f.split('.')[0] for f in os.listdir(db_path) if f.endswith('.dat')]\n",
        "    print(f\"Found {len(records)} VFDB records\")\n",
        "\n",
        "    for record in records:\n",
        "        try:\n",
        "            rec = wfdb.rdrecord(os.path.join(db_path, record))\n",
        "            if rec.p_signal is not None and rec.p_signal.shape[1] > 0:\n",
        "                sig = rec.p_signal[:, 0]\n",
        "                segments = preprocess_ecg(sig, fs=rec.fs)\n",
        "                if len(segments) > 0:\n",
        "                    X.extend(segments)\n",
        "                    y.extend([1] * len(segments))  # VT/VF\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {record}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"VFDB: Loaded {len(X)} segments\")\n",
        "    return X, y\n",
        "\n",
        "def load_nstdb(db_path):\n",
        "    X, y = [], []\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"Warning: {db_path} does not exist\")\n",
        "        return X, y\n",
        "\n",
        "    records = [f.split('.')[0] for f in os.listdir(db_path) if f.endswith('.dat')]\n",
        "    print(f\"Found {len(records)} NSTDB records\")\n",
        "\n",
        "    for record in records:\n",
        "        try:\n",
        "            rec = wfdb.rdrecord(os.path.join(db_path, record))\n",
        "            if rec.p_signal is not None and rec.p_signal.shape[1] > 0:\n",
        "                sig = rec.p_signal[:, 0]\n",
        "                segments = preprocess_ecg(sig, fs=rec.fs)\n",
        "                if len(segments) > 0:\n",
        "                    X.extend(segments)\n",
        "                    y.extend([2] * len(segments))  # Noisy\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {record}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"NSTDB: Loaded {len(X)} segments\")\n",
        "    return X, y\n",
        "\n",
        "def load_cuvtdb(db_path):\n",
        "    X, y = [], []\n",
        "    if not os.path.exists(db_path):\n",
        "        print(f\"Warning: {db_path} does not exist\")\n",
        "        return X, y\n",
        "\n",
        "    records = [f.split('.')[0] for f in os.listdir(db_path) if f.endswith('.dat')]\n",
        "    print(f\"Found {len(records)} CUVTDB records\")\n",
        "\n",
        "    for record in records:\n",
        "        try:\n",
        "            rec = wfdb.rdrecord(os.path.join(db_path, record))\n",
        "            if rec.p_signal is not None and rec.p_signal.shape[1] > 0:\n",
        "                sig = rec.p_signal[:, 0]\n",
        "                segments = preprocess_ecg(sig, fs=rec.fs)\n",
        "                if len(segments) > 0:\n",
        "                    X.extend(segments)\n",
        "                    y.extend([1] * len(segments))  # VF (same class as vfdb)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {record}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"CUVTDB: Loaded {len(X)} segments\")\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "VI0laye7yVhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "aCyaaADIl9P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ECG_CNN(nn.Module):\n",
        "    def __init__(self, input_length=1250, num_classes=3):\n",
        "        super(ECG_CNN, self).__init__()\n",
        "\n",
        "        # 5-layer CNN with 3x1 kernels\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(128)\n",
        "        self.pool3 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(256)\n",
        "        self.pool4 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv5 = nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.pool5 = nn.MaxPool1d(2)\n",
        "\n",
        "        # Calculate flattened size: 1250 -> 625 -> 312 -> 156 -> 78 -> 39\n",
        "        self.flattened_size = 512 * 39\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.flattened_size, 256)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure correct input shape\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(self.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool4(self.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool5(self.relu(self.bn5(self.conv5(x))))\n",
        "\n",
        "        # Flatten and FC layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "        features = self.relu(self.fc1(x))  # Store for t-SNE\n",
        "        features = self.dropout1(features)\n",
        "        x = self.relu(self.fc2(features))\n",
        "        x = self.dropout2(x)\n",
        "        output = self.fc3(x)\n",
        "\n",
        "        return output, features\n"
      ],
      "metadata": {
        "id": "jT0ndDb9l_c4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "LK8VpgQox2zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading datasets...\")\n",
        "    vf_X, vf_y = load_vfdb(VFDB_DIR)\n",
        "    cu_X, cu_y = load_cuvtdb(CUVTDB_DIR)\n",
        "    mit_X, mit_y = load_mitdb(ADB_DIR)\n",
        "    nst_X, nst_y = load_nstdb(NSTDB_DIR)\n",
        "\n",
        "    # Check if we have data\n",
        "    if len(vf_X) == 0 and len(cu_X) == 0 and len(mit_X) == 0 and len(nst_X) == 0:\n",
        "        print(\"No data loaded! Please check your data paths.\")\n",
        "        return\n",
        "\n",
        "    # Combine all segments and labels\n",
        "    all_X = vf_X + cu_X + mit_X + nst_X\n",
        "    all_y = (vf_y + cu_y + mit_y + nst_y)\n",
        "\n",
        "    if len(all_X) == 0:\n",
        "        print(\"No valid segments found!\")\n",
        "        return\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(all_X)\n",
        "    y = np.array(all_y)\n",
        "\n",
        "    print(f\"Total dataset: {X.shape}, Classes: {np.unique(y, return_counts=True)}\")\n",
        "\n",
        "    # Check for any remaining NaN or inf values\n",
        "    if np.isnan(X).any() or np.isinf(X).any():\n",
        "        print(\"Warning: NaN or inf values found in data!\")\n",
        "        # Remove problematic samples\n",
        "        valid_indices = ~(np.isnan(X).any(axis=1) | np.isinf(X).any(axis=1))\n",
        "        X = X[valid_indices]\n",
        "        y = y[valid_indices]\n",
        "        print(f\"After cleaning: {X.shape}\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42, stratify=y\n",
        "    )\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "    )\n",
        "\n",
        "    print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
        "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "    y_train_tensor = torch.LongTensor(y_train).to(device)\n",
        "    y_val_tensor = torch.LongTensor(y_val).to(device)\n",
        "    y_test_tensor = torch.LongTensor(y_test).to(device)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 32\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = ECG_CNN(input_length=X.shape[1], num_classes=len(np.unique(y))).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total model parameters: {total_params:,}\")\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 100\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss, train_correct = 0, 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(batch_X)\n",
        "\n",
        "            # Check for NaN in outputs\n",
        "            if torch.isnan(outputs).any():\n",
        "                print(f\"NaN detected in outputs at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            # Check for NaN in loss\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN detected in loss at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_correct += (outputs.argmax(1) == batch_y).sum().item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, val_correct = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                outputs, _ = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_correct += (outputs.argmax(1) == batch_y).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        train_acc = train_correct / len(train_dataset)\n",
        "        val_acc = val_correct / len(val_dataset)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhI3DLLOxzKX",
        "outputId": "026068b8-65af-4793-afa2-340c26c407ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading datasets...\n",
            "Found 22 VFDB records\n",
            "VFDB: Loaded 9240 segments\n",
            "Found 39 CUVTDB records\n",
            "CUVTDB: Loaded 707 segments\n",
            "Found 48 MITDB records\n",
            "MITDB: Loaded 17328 segments\n",
            "Found 30 NSTDB records\n",
            "NSTDB: Loaded 10830 segments\n",
            "Total dataset: (38105, 1250), Classes: (array([0, 1, 2]), array([17328,  9947, 10830]))\n",
            "Train: 26673, Val: 5716, Test: 5716\n",
            "Total model parameters: 5,670,659\n",
            "Starting training...\n",
            "Epoch [10/100]\n",
            "Train Loss: 0.0954, Train Acc: 0.9690\n",
            "Val Loss: 0.0889, Val Acc: 0.9745\n",
            "Epoch [20/100]\n",
            "Train Loss: 0.0726, Train Acc: 0.9746\n",
            "Val Loss: 0.0626, Val Acc: 0.9750\n",
            "Epoch [30/100]\n",
            "Train Loss: 0.0625, Train Acc: 0.9768\n",
            "Val Loss: 0.0612, Val Acc: 0.9762\n",
            "Epoch [40/100]\n",
            "Train Loss: 0.0626, Train Acc: 0.9759\n",
            "Val Loss: 0.0720, Val Acc: 0.9741\n",
            "Epoch [50/100]\n",
            "Train Loss: 0.0619, Train Acc: 0.9760\n",
            "Val Loss: 0.0706, Val Acc: 0.9750\n",
            "Epoch [60/100]\n",
            "Train Loss: 0.0586, Train Acc: 0.9775\n",
            "Val Loss: 0.0504, Val Acc: 0.9780\n",
            "Epoch [70/100]\n",
            "Train Loss: 0.0577, Train Acc: 0.9768\n",
            "Val Loss: 0.0792, Val Acc: 0.9701\n",
            "Epoch [80/100]\n",
            "Train Loss: 0.0557, Train Acc: 0.9781\n",
            "Val Loss: 0.0661, Val Acc: 0.9757\n",
            "Epoch [90/100]\n",
            "Train Loss: 0.0522, Train Acc: 0.9791\n",
            "Val Loss: 0.1095, Val Acc: 0.9710\n",
            "Epoch [100/100]\n",
            "Train Loss: 0.0577, Train Acc: 0.9792\n",
            "Val Loss: 0.0667, Val Acc: 0.9762\n",
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KSXYdWsTs6ng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}